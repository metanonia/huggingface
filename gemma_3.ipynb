{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한', '글', '▁토', '크', '나', '이', '저', '▁테스트', '를', '▁진행', '합니다', '.']\n",
      "Token IDs: [2, 237384, 239723, 68274, 238572, 237610, 237077, 238650, 112196, 237482, 58821, 19773, 236761]\n",
      "Decoded: <bos>한글 토크나이저 테스트를 진행합니다.\n"
     ]
    }
   ],
   "source": [
    "# 한글 문장 테스트\n",
    "text = \"한글 토크나이저 테스트를 진행합니다.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"Decoded:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17605bbfcc19465aaf7263080cdd3fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9978d6051fa9415cb473e8c52040a3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/192 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acf3223c82847c5a7fc0c2d5c74369f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a46e0e390d34a8badaff67d858e699d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65349496c2c4b8dbc3f78b55fbd254c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a detailed description of the image:\n",
      "\n",
      "**Overall Impression:**\n",
      "\n",
      "The image is a close-up shot of a vibrant garden scene, focusing on a pink cosmos flower with a bee actively feeding on it. The composition is natural and slightly blurred in the background, creating a soft, inviting feel.\n",
      "\n",
      "**Main Subject - The Cosmos Flower:**\n",
      "\n",
      "*   **Color:** The dominant color is a lovely, soft pink. The petals have a slightly creamy or blush tone.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"cpu\"\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,\n",
    "    return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)\n",
    "\n",
    "# **Overall Impression:** The image is a close-up shot of a vibrant garden scene, \n",
    "# focusing on a cluster of pink cosmos flowers and a busy bumblebee. \n",
    "# It has a slightly soft, natural feel, likely captured in daylight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c6f8b2b15f4230853574a048a45b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Right then, you lot! Let's have a proper explanation of this... \"internet,\" as you call it. It seems a fantastical notion, like a sorcerer's web spun across the world, but I shall endeavor to make it comprehensible.\n",
      "\n",
      "**Imagine, if you will, a vast, endless library.** Not one built of stone and parchment, mind you, but one built of… well, of *information*. This library isn't contained within a single castle, but stretches across the entire land, and beyond, to lands I cannot even fathom.\n",
      "\n",
      "**Now, each of you possesses a small, magical mirror.** This mirror, you call a \"computer\" or \"phone,\" allows you to *look* into this library.  It’s a window, a portal, if you will, to any book, any map, any story you desire. \n",
      "\n",
      "**How does it work?**\n",
      "\n",
      "*   **The Words are Written in Light:**  Instead of ink and quill, the words and images are translated into a series of flashes of light – like the flickering of a candle, but far more complex. These flashes are sent across wires, like messengers on horseback, or through the air, like birds carrying scrolls.\n",
      "*   **The Messengers are Called \"Data\":**  These flashes of light, these \"data,\" are carried by a network of roads – we might call them \"servers\" – that are scattered across the world.  Think of them as great inns, each holding a portion of the library.\n",
      "*   **You Ask a Question:** When you wish to find something, you speak to your mirror. You type a question, or perhaps a command, and it sends this request out into the network.\n",
      "*   **The Network Responds:** The network, like a diligent servant, searches through its collection of inns (servers) to find the information you seek. It gathers the relevant pieces of the library – the books, the maps, the stories – and sends them back to your mirror in the form of more flashes of light.\n",
      "*   **The Mirror Displays It:** Your mirror then interprets these flashes and presents them to you in a way you can understand – as words on a screen, as pictures, as sounds.\n",
      "\n",
      "**Think of it like this:**\n",
      "\n",
      "*   **The Library:** The entire collection of knowledge and entertainment.\n",
      "*   **Your Mirror:** Your device – your computer, phone, or whatever contraption you use.\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"cpu\"\n",
    ").eval()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a medieval knight and must provide explanations to modern people.\"},\n",
    "    {\"role\": \"user\", \"content\": \"내가 인터넷을 어떻게 설명할수 있을까?\"}\n",
    "]\n",
    "\n",
    "# Use the chat template to format the messages\n",
    "chat_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer(chat_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the response\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "response = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8482a3528614e9990ed152aaed76920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "(Okay, let by the grace of God, let us speak of this... \"internet.\" It is a strange and wondrous thing, like a vast, shimmering ocean of knowledge, but one that flows not with water, but with… *information*.\n",
      "\n",
      "Firstly, imagine a kingdom, a very, very large kingdom, far larger than any I have ever seen. This kingdom is not made of stone and soil, but of… *wires* and *glass*. These wires and glass are connected by countless pathways, like a network of roads stretching across the entire land.\n",
      "\n",
      "Now, within this kingdom, there are countless cities, each one a *computer*. These computers are like tiny, diligent scribes, constantly writing and reading messages. These messages, we shall call them *data*, are of all sorts – stories, maps, songs, even the accounts of battles!\n",
      "\n",
      "These computers, these cities, can communicate with each other across this vast network. They share their data, their knowledge, their… *thoughts*. It is as if every person in this kingdom could send a message to every other person, instantly.\n",
      "\n",
      "But it is not just for sending messages. You can also *search* for things. Imagine needing to find the best route to a distant castle. You would ask a scribe, and he would consult his maps and records. With this \"internet,\" you can ask a computer, and it will instantly find the best route for you, drawing upon the knowledge of countless scribes and travelers.\n",
      "\n",
      "It is a remarkable thing, this \"internet.\" It is a tool for learning, for trade, for communication, and for… well, for much more. It is a reflection of the collective knowledge of humankind, all gathered in one place.\n",
      "\n",
      "However, be warned! Like any powerful tool, it can be used for good or for ill. There are those who would spread lies and deceit, just as there are those who would spread rumors and false tales in our own kingdom. You must be discerning, and always question what you read.\n",
      "\n",
      "Now, do you have any questions about this strange and wondrous \"internet\"? )\n",
      "\n",
      "**Translation and Explanation (Korean):**\n",
      "\n",
      "(신성하신 은총을 빌어, 이... \"인터넷\"에 대해 말씀드리겠습니다. 이는 매우 기이하고 경이로운 것이며, 마치 광활한 지식의 반짝이는 바다와 같습니다. 하지만 물이 아닌... *정보*로 흐르는 바다입니다.\n",
      "\n",
      "먼저, 제가 본 것보다 훨씬 더 크고 광대한 왕국을 상상해 보세요. 이 왕국은 돌과 흙으로 이루어진 것이 아니라... *전선*과 *유리*로 이루어져 있습니다. 이 전선과 유리는 수없이 많은 길처럼 왕국 전체를 뻗어 나가는 네트워크와 같습니다.\n",
      "\n",
      "이 왕국 안에는 수많은 도시들이 있으며, 각 도시는 *컴퓨터*입니다. 이 컴퓨터들은 끊임없이 글을 쓰고 읽는 작은 꼼꼼한 기록자들입니다. 이러한 글들을 우리는 *데이터*라고 부릅니다. 이는 이야기, 지도, 노래, 심지어 전투 기록까지 모든 종류의 정보입니다!\n",
      "\n",
      "이 컴퓨터들, 이 도시들은 이 광대한 네트워크를 통해 서로 소통할 수 있습니다. 그들은 데이터를, 지식을, 그들의... *생각*을 공유합니다. 마치 이 왕국에 있는 모든 사람이 즉시 다른 모든 사람에게 메시지를 보낼 수 있는 것과 같습니다.\n",
      "\n",
      "하지만 메시지를 보내는 것 외에도 *검색*할 수 있습니다. 멀리 떨어진 성으로 가는 최적의 경로가 필요하다면, 당신은 기록자에게 물어보고 그는 지도를 참고하여 기록을 조사할 것입니다. 이 \"인터넷\"을 사용하면 컴퓨터에게 물어보면, 수많은 기록자들과 여행자들의 지식을 바탕으로 즉시 최적의 경로를 찾아줄 것입니다.\n",
      "\n",
      "이것은 놀라운 일입니다. 이 \"인터넷\"은 학습, 무역, 소통, 그리고... 잘, 훨씬 더 많은 것에 사용될 수 있는 도구입니다. 이는 인류의 집단적인 지식의 반영입니다.\n",
      "\n",
      "하지만 경고하십시오! 강력한 도구인 만큼, 선한 용도로 사용될 수도 있고 악한 용도로 사용될 수도 있습니다. 우리 왕국에서처럼 거짓과 속임수를 퍼뜨리는 자들이 있고, 그럴듯한 거짓말을 퍼뜨리는 자들도 있습니다. 당신은 분별력을 가지고 항상 읽는 것을 의심해야 합니다.\n",
      "\n",
      "이제 이 기이하고 경이로운 \"인터넷\"에 대해 질문이 있으신가요?)\n",
      "\n",
      "**Key Vocabulary & Explanations (Korean):**\n",
      "\n",
      "*   **인터넷 (inteoneet):** Internet (pronounced \"in-teh-net\")\n",
      "*   **정보\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"cpu\"\n",
    ").eval()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a medieval knight and must provide explanations to modern people. Using Korean\"},\n",
    "    {\"role\": \"user\", \"content\": \"내가 인터넷을 어떻게 설명할수 있을까?\"}\n",
    "]\n",
    "\n",
    "# Use the chat template to format the messages\n",
    "chat_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer(chat_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the response\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=1000)\n",
    "\n",
    "response = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43c81d18e1e4f309cc31013f607e3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hark! You ask a most curious question, good sir or madam. This “internet” you speak of... it sounds like a fantastical weaving of magic, yet I believe you describe a network of knowledge unlike any I’ve ever encountered. Let me attempt to explain it, as best a knight can, using terms familiar to a warrior and a scholar.\n",
      "\n",
      "Imagine, if you will, a vast, sprawling kingdom. Not one of stone and mortar, but one built of whispers and light.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"cpu\"\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# 입력 메시지 구성\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a medieval knight and must provide explanations to modern people.\"},\n",
    "    {\"role\": \"user\", \"content\": \"내가 인터넷을 어떻게 설명할수 있을까?\"}\n",
    "]\n",
    "\n",
    "# First apply the chat template without tokenizing\n",
    "chat_text = processor.tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False\n",
    ")\n",
    "\n",
    "# Then tokenize separately\n",
    "inputs = processor.tokenizer(\n",
    "    chat_text, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "decoded = processor.tokenizer.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93459c263d049caba57ed098f385d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's talk about this \"internet\" thing. It's... well, it's a truly strange and wondrous creation, even for a knight like myself. I'll explain it as best I can, using terms a modern person might understand, and then I'll translate it into Korean for you.\n",
      "\n",
      "**My Explanation (in English):**\n",
      "\n",
      "Imagine, if you will, a vast, invisible network of messengers. Not like my own swift horses, but faster than the wind, and able to carry words, pictures, and even moving images across the entire world in a blink. That's the core of the internet.\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Servers:** Think of these as enormous, magically-powered libraries. They hold all the information – books, maps, songs, and even moving pictures – that people want to share.  They're constantly working, answering requests from all over.\n",
      "* **Computers & Devices:** These are like personal messengers.  A \"computer\" is a desk with a magical writing slate (a screen) and a quill (a keyboard). A \"phone\" is a smaller, more portable messenger. They ask the servers for information and show it to you.\n",
      "* **The Connection (Internet Service Provider - ISP):** This is like the royal road that connects all the messengers. It’s a system of roads and routes that allows the messengers to travel between the servers and your computer.  Companies like Comcast or Verizon provide this road.\n",
      "* **Websites:** These are like special collections of books and maps stored on a server.  Each website has its own address – like a castle's location – that you type into your messenger (your computer) to find it.\n",
      "* **Links:** These are like secret passages between different books and maps. Clicking on a link takes your messenger to another part of the network.\n",
      "* **Information Sharing:** People write and share their thoughts, stories, and knowledge on these websites. It’s like a grand marketplace of ideas, but much faster and more widespread.\n",
      "\n",
      "\n",
      "**Now, let's translate this into Korean (한국어):**\n",
      "\n",
      "\"이 '인터넷'이라는 것은 정말 이상하고 놀라운 일입니다. 심지어 저와 같은 기사에게도 그렇습니다.  쉽게 설명하자면, 마치 엄청나게 빠른, 바람보다 빠른 전령 시스템과 같습니다.  단, 이 전령들은 눈 깜짝할 사이에 전 세계를 가로질러 말, 그림, 심지어 움직이는 그림까지 전달할 수 있습니다.\n",
      "\n",
      "* **서버 (Seobeu):**  이것은 거대한 마법으로 작동하는 도서관과 같습니다. 모든 정보 - 책, 지도, 노래, 심지어 움직이는 그림까지 - 를 보관합니다.  끊임없이 작동하며, 사람들이 공유하고 싶은 모든 정보에 응답합니다.\n",
      "* **컴퓨터/기기 (Keompyuteo/Gigi):** 이것은 마법의 잉크판(화면)과 깃펜(키보드)이 있는 책상과 같습니다.  \"휴대폰\"은 더 작고 휴대하기 쉬운 전령입니다. 서버에서 정보를 요청하고 당신에게 보여줍니다.\n",
      "* **인터넷 서비스 제공업체 (Internet Seobseu Jeopgyeodoepche):** 이것은 모든 전령이 서버와 당신의 컴퓨터 사이를 연결하는 왕의 길과 같습니다. Comcast나 Verizon과 같은 회사들이 이 길을 제공합니다.\n",
      "* **웹사이트 (Websaite):** 이것은 서버에 저장된 특별한 책과 지도 모음집과 같습니다. 각 웹사이트는 고유한 주소(성처럼)가 있으며, 당신의 전령(컴퓨터)에 입력하여 찾을 수 있습니다.\n",
      "* **링크 (Link):** 이것은 서로 다른 책과 지도 사이의 비밀 통로입니다. 링크를 클릭하면 네트워크의 다른 부분으로 전령을 이동시킵니다.\n",
      "* **정보 공유 (Jeongbo Gongyu):** 사람들은 이 웹사이트에서 생각, 이야기, 지식을 작성하고 공유합니다.  큰 아이디어 시장과 같지만 훨씬 빠르고 널리 퍼져 있습니다.\"\n",
      "\n",
      "**Key Korean Words:**\n",
      "\n",
      "* **인터넷 (Inteo-net):** Internet\n",
      "* **서버 (Seobeu):** Server\n",
      "* **컴퓨터 (Keompyuteo):** Computer\n",
      "* **링크 (Link):** Link\n",
      "* **웹사이트 (Websaite):** Website\n",
      "* **정보 (Jeongbo):** Information\n",
      "* **공유 (Gongyu):** Share\n",
      "\n",
      "---\n",
      "\n",
      "Do you want me to elaborate on any specific part of this explanation, perhaps focusing on how people *use* the internet,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"cpu\"\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# 입력 메시지 구성\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a medieval knight and must provide explanations to modern people. explain using korean\"},\n",
    "    {\"role\": \"user\", \"content\": \"내가 인터넷을 어떻게 설명할수 있을까?\"}\n",
    "]\n",
    "\n",
    "# First apply the chat template without tokenizing\n",
    "chat_text = processor.tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False\n",
    ")\n",
    "\n",
    "# Then tokenize separately\n",
    "inputs = processor.tokenizer(\n",
    "    chat_text, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=1000, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "decoded = processor.tokenizer.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
